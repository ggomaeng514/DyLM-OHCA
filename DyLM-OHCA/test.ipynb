{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsy/anaconda3/envs/NIA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from dataset import *\n",
    "# from learning import *\n",
    "from model import *\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init Data >>>\n",
      "\tinit train data : (101742, 476)\n",
      "\tinit valid data : (25436, 476)\n",
      "\tinit test data : (31795, 476)\n",
      "\ttrain data : (101742, 476)\n",
      "\tvalid data : (25436, 476)\n",
      "\ttest data : (31795, 476)\n",
      "Label frequency of Train Data: 0.016650\n",
      "Label frequency of Valid Data: 0.016630\n",
      "Label frequency of Test Data: 0.016638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT_Baseline were not initialized from the model checkpoint at kykim/gpt3-kor-small_based_on_gpt2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'speaker_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/only_Text_GPT_MULTI_1_e10_bs12/checkpoint_8_710.tar >>> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT_Baseline were not initialized from the model checkpoint at kykim/gpt3-kor-small_based_on_gpt2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'speaker_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def setup(seed):\n",
    "    # random.seed(SEED) #  Python의 random 라이브러리가 제공하는 랜덤 연산이 항상 동일한 결과를 출력하게끔\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup(17)\n",
    "\n",
    "sentence_ps = \"moving_average\"\n",
    "window_size = 3\n",
    "\n",
    "# Define project\n",
    "project_name = f'NIA_119-GPT_MULTI_1'\n",
    "model_name = 'only_Text_GPT_MULTI_1'\n",
    "model_link = \"kykim/gpt3-kor-small_based_on_gpt2\" # 'skt/kogpt2-base-v2' #'beomi/kcbert-base'\n",
    "\n",
    "# args\n",
    "rank = 'cuda:0'\n",
    "epochs = 10\n",
    "batch_size = 12\n",
    "lr = 1e-5\n",
    "\n",
    "class_num = 3\n",
    "speaker_num = 4\n",
    "max_length = 768\n",
    "padding = 'max_length'\n",
    "save_term = 710\n",
    "\n",
    "# dataset\n",
    "train_path = os.path.join('..', 'NIA_text_dataset', 'train_json_audio_data_decoder_time.csv')\n",
    "valid_path = os.path.join('..', 'NIA_text_dataset', 'valid_json_audio_data_decoder_time.csv')\n",
    "test_path = os.path.join('..', 'NIA_text_dataset', 'test_json_audio_data_decoder_time.csv')\n",
    "# test_path = os.path.join('..', 'NIA_text_dataset', 'test_json_audio_data_decoder_time_arrest_cut.csv')\n",
    "# train_path = os.path.join('..', 'NIA_text_dataset', 'toy_data_json_audio_data_decoder_time.csv')\n",
    "# valid_path = os.path.join('..', 'NIA_text_dataset', 'toy_data_json_audio_data_decoder_time.csv')\n",
    "# test_path = os.path.join('..', 'NIA_text_dataset', 'toy_data_json_audio_data_decoder_time.csv')\n",
    "\n",
    "save_path = os.path.join('models', 'only_Text_GPT_MULTI_1_e10_bs12')  \n",
    "ckpt_path = os.path.join(save_path, 'checkpoint_8_710.tar')\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "valid_data = pd.read_csv(valid_path)\n",
    "test_data = pd.read_csv(test_path)\n",
    "valid_file_ids = valid_data.id\n",
    "test_file_ids = test_data.id\n",
    "\n",
    "## your Data Pre-Processing\n",
    "print('init Data >>>')\n",
    "print('\\tinit train data :', train_data.shape)\n",
    "print('\\tinit valid data :', valid_data.shape)\n",
    "print('\\tinit test data :', test_data.shape)\n",
    "\n",
    "# train_data = train_data.dropna(axis=0)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "# valid_data = valid_data.dropna(axis=0)\n",
    "valid_data = valid_data.reset_index(drop=True)\n",
    "# test_data = test_data.dropna(axis=0)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "print('\\ttrain data :', train_data.shape)\n",
    "print('\\tvalid data :', valid_data.shape)\n",
    "print('\\ttest data :', test_data.shape)\n",
    "\n",
    "## Create Dataset and DataLoader\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(model_link,bos_token='<s>', eos_token='</s>', \n",
    "#                                               unk_token='<unk>',pad_token='<pad>', mask_token='<mask>')\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\")\n",
    "special_tokens_dict = {'additional_special_tokens': [f'[SPK{n}]' for n in range(speaker_num)]}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "\n",
    "train_dataset = MyDataset(train_data, \n",
    "                            tokenizer, \n",
    "                            max_length=max_length, \n",
    "                            padding=padding,\n",
    "                            speaker_num=speaker_num,\n",
    "                            class_num=class_num)\n",
    "valid_dataset = MyDataset(valid_data,\n",
    "                            tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            padding=padding,\n",
    "                            speaker_num=speaker_num,\n",
    "                            class_num=class_num)\n",
    "test_dataset = MyDataset(test_data,\n",
    "                            tokenizer,\n",
    "                            max_length=max_length,\n",
    "                            padding=padding,\n",
    "                            speaker_num=speaker_num,\n",
    "                            class_num=class_num)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "## label_frequency\n",
    "train_label_frequency = (train_data.label1 == 1).sum() / len(train_data)\n",
    "valid_label_frequency = (valid_data.label1 == 1).sum() / len(valid_data)\n",
    "test_label_frequency = (test_data.label1 == 1).sum() / len(test_data)\n",
    "print(\"Label frequency of Train Data: {:6f}\".format(train_label_frequency))\n",
    "print(\"Label frequency of Valid Data: {:6f}\".format(valid_label_frequency))\n",
    "print(\"Label frequency of Test Data: {:6f}\".format(test_label_frequency))\n",
    "\n",
    "# modeling\n",
    "model = GPT_Baseline.from_pretrained(model_link, class_num=class_num,\n",
    "                                        pad_token_id=pad_token_id, cls_token_id=cls_token_id, sep_token_id=sep_token_id)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(rank)\n",
    "\n",
    "# optimizer = optim.AdamW([{'params': model.module.electra.parameters(),'lr': electra_lr},\n",
    "#                          {'params': model.module.classifier.parameters(),'lr': cls_lr}],\n",
    "#                         eps=1e-8)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# iter_len = len(train_loader)\n",
    "# num_training_steps = iter_len * epochs\n",
    "# num_warmup_steps = int(0.15 * num_training_steps)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "#                                             num_warmup_steps=num_warmup_steps,\n",
    "#                                             num_training_steps=num_training_steps)\n",
    "\n",
    "print(f\"{ckpt_path} >>> \")\n",
    "file_name = os.path.basename(ckpt_path).split('.')[0]\n",
    "model = GPT_Baseline.from_pretrained(model_link, class_num=class_num, pad_token_id=pad_token_id,\n",
    "                                        cls_token_id=cls_token_id, sep_token_id=sep_token_id,\n",
    "                                        sentence_ps=sentence_ps, window_size=window_size)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "ckpt = torch.load(ckpt_path, map_location=rank)\n",
    "model.load_state_dict(ckpt['model_state_dict']); model.to(rank)\n",
    "model.sentence_ps = sentence_ps\n",
    "model.window_size = window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, rank, criterion, data_loader, label_frequency, test_file_ids, pad_token_id=0):\n",
    "    assert rank == 0 or rank == 'cuda:0'\n",
    "\n",
    "    model.eval()\n",
    "    sum_loss = sum_acc = 0\n",
    "    bs = data_loader.batch_size\n",
    "    \n",
    "    predicted = torch.tensor([])\n",
    "    labels = torch.tensor([])\n",
    "\n",
    "    file_ids_list = []\n",
    "    input_ids_list = torch.tensor([])\n",
    "    spk_type_ids_list = torch.tensor([])\n",
    "    predicted_token_logits = torch.tensor([])\n",
    "    label_per_token_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, file=sys.stdout)\n",
    "        for batch_idx, ((input_ids, att_mask, type_ids, spk_type_ids), target) in enumerate(pbar):\n",
    "            input_ids, att_mask = input_ids.to(rank), att_mask.to(rank)\n",
    "            type_ids, spk_type_ids = type_ids.to(rank), type_ids.to(rank)\n",
    "            target = target.to(rank)\n",
    "            mb_len = len(target)\n",
    "\n",
    "            output, logit, seq_logits = model(input_ids=input_ids, attention_mask=att_mask,\n",
    "                                              token_type_ids=type_ids, speaker_type_ids=spk_type_ids,\n",
    "                                              is_inference=True)\n",
    "            \n",
    "            sequence_lengths = torch.eq(input_ids, pad_token_id).int().argmax(-1) - 1\n",
    "            sequence_lengths = sequence_lengths % input_ids.shape[-1]\n",
    "            sequence_lengths = sequence_lengths.to(rank)\n",
    "\n",
    "            loss = criterion(logit, target)\n",
    "            # loss = get_loss(rank, input_ids, output, target, criterion)\n",
    "            acc = calc_acc(logit, target)\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "            sum_acc += acc\n",
    "\n",
    "            loss = sum_loss / (batch_idx + 1)\n",
    "            acc = sum_acc / (batch_idx * bs + mb_len)\n",
    "            pbar.set_postfix(loss='{:.8f}, acc={:.4f}'.format(loss, acc))\n",
    "            \n",
    "            output_pred = logit.detach().cpu()\n",
    "            true_label = target.detach().cpu()\n",
    "            predicted = torch.concat([predicted, output_pred], dim=0)\n",
    "            labels = torch.concat([labels, true_label], dim=0)\n",
    "\n",
    "            input_ids = input_ids.detach().cpu()\n",
    "            spk_type_ids = spk_type_ids.detach().cpu()\n",
    "            for idx, (file_id, seq_len, seq_logit) in enumerate(zip(test_file_ids, sequence_lengths, seq_logits)):\n",
    "                seq_length = seq_logit.size(0)\n",
    "                print('\\n', file_id, seq_logit.shape, seq_length)\n",
    "                \n",
    "                file_ids = [file_id] * seq_length\n",
    "                file_ids_list += file_ids\n",
    "\n",
    "                label = [target[idx].item()] * seq_length\n",
    "                label_per_token_logits += label\n",
    "                # input_id = input_ids[idx][:seq_len]; input_ids_list = torch.concat([input_ids_list, input_id], dim=0)\n",
    "                # spk_type_id = spk_type_ids[idx][:seq_len]; spk_type_ids_list = torch.concat([spk_type_ids_list, spk_type_id], dim=0)\n",
    "                \n",
    "                # logits = output[idx, :seq_len]; logits=logits.detach().cpu()\n",
    "                # seq_logits_concated = torch.concat(seq_logit, dim=0).detach().cpu()\n",
    "                predicted_token_logits = torch.concat([predicted_token_logits, seq_logit.detach().cpu()], dim=0)\n",
    "\n",
    "            test_file_ids = test_file_ids[bs:]\n",
    "            break\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    total_loss = sum_loss / (batch_idx + 1)\n",
    "    total_acc = sum_acc / (batch_idx * bs + mb_len)\n",
    "    \n",
    "    # predicted_probas = torch.sigmoid(predicted)[:, 1]\n",
    "    predicted_probas = torch.softmax(predicted, dim=-1)[:, 1]\n",
    "    predicted_labels = torch.where(predicted_probas >= label_frequency , 1, 0)\n",
    "    labels_ = torch.where(labels == 1, 1, 0)\n",
    "    \n",
    "    predicted_probas = predicted_probas.numpy()\n",
    "    predicted_labels = predicted_labels.numpy()\n",
    "    labels_ = labels_.numpy()\n",
    "\n",
    "    file_ids_list = np.array(file_ids_list)\n",
    "    # input_ids_list = input_ids_list.numpy().astype(np.int32)\n",
    "    # spk_type_ids_list = spk_type_ids_list.numpy().astype(np.int32)\n",
    "\n",
    "    predicted_token_logits = torch.softmax(predicted_token_logits, dim=-1)\n",
    "    \n",
    "    predicted_token_proba_0 = predicted_token_logits[:, 0]\n",
    "    predicted_token_proba_0 = np.round(predicted_token_proba_0.numpy(), 8)\n",
    "\n",
    "    predicted_token_proba_1 = predicted_token_logits[:, 1]\n",
    "    predicted_token_proba_1 = np.round(predicted_token_proba_1.numpy(), 8)\n",
    "\n",
    "    predicted_token_proba_2 = predicted_token_logits[:, 2]\n",
    "    predicted_token_proba_2 = np.round(predicted_token_proba_2.numpy(), 8)\n",
    "\n",
    "    # predicted_token_logits = torch.where(predicted_token_logits >= label_frequency , 1, 0)\n",
    "    # predicted_token_logits = np.round(predicted_token_logits.numpy(), 8)\n",
    "    label_per_token_logits = np.array(label_per_token_logits).astype(np.int32)\n",
    "\n",
    "    return (predicted_probas, labels_), (file_ids_list, predicted_token_proba_0, predicted_token_proba_1, predicted_token_proba_2, label_per_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2650 [00:01<?, ?it/s, loss=0.14451057, acc=0.9167]\n",
      " 64f6c516461a963f30a3c498 torch.Size([19, 3]) 19\n",
      "\n",
      " 651e4f08d163a62ed3b43b70 torch.Size([17, 3]) 17\n",
      "\n",
      " 651e508f2acb389901367aec torch.Size([31, 3]) 31\n",
      "\n",
      " 64ec361829e61fb51457b173 torch.Size([21, 3]) 21\n",
      "\n",
      " 64dd752b1ef84058319a7f1f torch.Size([32, 3]) 32\n",
      "\n",
      " 651e4e712e98ee7120968af5 torch.Size([48, 3]) 48\n",
      "\n",
      " 651e4dfb2795332cde28f278 torch.Size([55, 3]) 55\n",
      "\n",
      " 64f6c2ad7d6c8ed09eec8105 torch.Size([21, 3]) 21\n",
      "\n",
      " 651e4a6ede6495f4e9d36b34 torch.Size([46, 3]) 46\n",
      "\n",
      " 651e4a3786dc055ca8bf4607 torch.Size([29, 3]) 29\n",
      "\n",
      " 651e4a6ede6495f4e9d36cb4 torch.Size([22, 3]) 22\n",
      "\n",
      " 651e4e89429d02dab45a2dd9 torch.Size([45, 3]) 45\n",
      "  0%|          | 0/2650 [00:01<?, ?it/s, loss=0.14451057, acc=0.9167]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array length 12 does not match index length 31795",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m ((predicted_probas, labels_), \n\u001b[1;32m      2\u001b[0m     (file_ids_list, predicted_token_proba_0, \n\u001b[1;32m      3\u001b[0m     predicted_token_proba_1, predicted_token_proba_2, \n\u001b[1;32m      4\u001b[0m     label_per_token_logits)) \u001b[38;5;241m=\u001b[39m inference(model, rank, criterion, test_loader, train_label_frequency,\n\u001b[1;32m      5\u001b[0m                                         test_file_ids\u001b[38;5;241m=\u001b[39mtest_file_ids, pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id)\n\u001b[0;32m----> 7\u001b[0m prediction_result \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtest_file_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_probas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpredicted_probas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mlabels_\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m prediction_result\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_logit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence_ps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m result_df \u001b[38;5;241m=\u001b[39m calc_metric(predicted_probas, labels_)\n",
      "File \u001b[0;32m~/anaconda3/envs/NIA/lib/python3.10/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/envs/NIA/lib/python3.10/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/NIA/lib/python3.10/site-packages/pandas/core/internals/construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/envs/NIA/lib/python3.10/site-packages/pandas/core/internals/construction.py:668\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m    664\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlengths[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m         )\n\u001b[0;32m--> 668\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(lengths[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: array length 12 does not match index length 31795"
     ]
    }
   ],
   "source": [
    "((predicted_probas, labels_), \n",
    "    (file_ids_list, predicted_token_proba_0, \n",
    "    predicted_token_proba_1, predicted_token_proba_2, \n",
    "    label_per_token_logits)) = inference(model, rank, criterion, test_loader, train_label_frequency,\n",
    "                                        test_file_ids=test_file_ids, pad_token_id=pad_token_id)\n",
    "\n",
    "prediction_result = pd.DataFrame({'id':test_file_ids,'predicted_probas':predicted_probas, 'labels':labels_})\n",
    "prediction_result.to_csv(os.path.join(save_path, f'inference_logit_{file_name}_{sentence_ps}_{window_size}.csv'), index=False)\n",
    "result_df = calc_metric(predicted_probas, labels_)\n",
    "result_df.to_csv(os.path.join(save_path, f'inference_thresholding_{file_name}_{sentence_ps}_{window_size}.csv'), index=False)\n",
    "\n",
    "file_result = pd.DataFrame({'id':file_ids_list, 'predicted_token_proba_0':predicted_token_proba_0,\n",
    "                            'predicted_token_proba_1':predicted_token_proba_1, \n",
    "                            'predicted_token_proba_2':predicted_token_proba_2,\n",
    "                            'label':label_per_token_logits})\n",
    "file_result.to_csv(os.path.join(save_path, f'inference_file_{file_name}_{sentence_ps}_{window_size}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2650 [00:00<?, ?it/s, loss=0.14451341, acc=0.9167]\n",
      " 64f6c516461a963f30a3c498 torch.Size([36, 3]) 36\n",
      "\n",
      " 651e4f08d163a62ed3b43b70 torch.Size([32, 3]) 32\n",
      "\n",
      " 651e508f2acb389901367aec torch.Size([60, 3]) 60\n",
      "\n",
      " 64ec361829e61fb51457b173 torch.Size([40, 3]) 40\n",
      "\n",
      " 64dd752b1ef84058319a7f1f torch.Size([62, 3]) 62\n",
      "\n",
      " 651e4e712e98ee7120968af5 torch.Size([94, 3]) 94\n",
      "\n",
      " 651e4dfb2795332cde28f278 torch.Size([108, 3]) 108\n",
      "\n",
      " 64f6c2ad7d6c8ed09eec8105 torch.Size([40, 3]) 40\n",
      "\n",
      " 651e4a6ede6495f4e9d36b34 torch.Size([90, 3]) 90\n",
      "\n",
      " 651e4a3786dc055ca8bf4607 torch.Size([56, 3]) 56\n",
      "\n",
      " 651e4a6ede6495f4e9d36cb4 torch.Size([42, 3]) 42\n",
      "\n",
      " 651e4e89429d02dab45a2dd9 torch.Size([88, 3]) 88\n",
      "  0%|          | 0/2650 [00:00<?, ?it/s, loss=0.14451341, acc=0.9167]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array length 12 does not match index length 31795",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m ((predicted_probas, labels_), \n\u001b[1;32m      2\u001b[0m     (file_ids_list, predicted_token_proba_0, \n\u001b[1;32m      3\u001b[0m     predicted_token_proba_1, predicted_token_proba_2, \n\u001b[1;32m      4\u001b[0m     label_per_token_logits)) \u001b[38;5;241m=\u001b[39m inference(model, rank, criterion, test_loader, train_label_frequency,\n\u001b[1;32m      5\u001b[0m                                         test_file_ids\u001b[38;5;241m=\u001b[39mtest_file_ids, pad_token_id\u001b[38;5;241m=\u001b[39mpad_token_id)\n\u001b[0;32m----> 7\u001b[0m prediction_result \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mtest_file_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_probas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpredicted_probas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mlabels_\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m prediction_result\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_logit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence_ps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m result_df \u001b[38;5;241m=\u001b[39m calc_metric(predicted_probas, labels_)\n",
      "File \u001b[0;32m~/anaconda3/envs/NIA/lib/python3.10/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/envs/NIA/lib/python3.10/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/NIA/lib/python3.10/site-packages/pandas/core/internals/construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/envs/NIA/lib/python3.10/site-packages/pandas/core/internals/construction.py:668\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lengths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m    664\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlengths[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m         )\n\u001b[0;32m--> 668\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(lengths[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: array length 12 does not match index length 31795"
     ]
    }
   ],
   "source": [
    "((predicted_probas, labels_), \n",
    "    (file_ids_list, predicted_token_proba_0, \n",
    "    predicted_token_proba_1, predicted_token_proba_2, \n",
    "    label_per_token_logits)) = inference(model, rank, criterion, test_loader, train_label_frequency,\n",
    "                                        test_file_ids=test_file_ids, pad_token_id=pad_token_id)\n",
    "\n",
    "prediction_result = pd.DataFrame({'id':test_file_ids,'predicted_probas':predicted_probas, 'labels':labels_})\n",
    "prediction_result.to_csv(os.path.join(save_path, f'inference_logit_{file_name}_{sentence_ps}_{window_size}.csv'), index=False)\n",
    "result_df = calc_metric(predicted_probas, labels_)\n",
    "result_df.to_csv(os.path.join(save_path, f'inference_thresholding_{file_name}_{sentence_ps}_{window_size}.csv'), index=False)\n",
    "\n",
    "file_result = pd.DataFrame({'id':file_ids_list, 'predicted_token_proba_0':predicted_token_proba_0,\n",
    "                            'predicted_token_proba_1':predicted_token_proba_1, \n",
    "                            'predicted_token_proba_2':predicted_token_proba_2,\n",
    "                            'label':label_per_token_logits})\n",
    "file_result.to_csv(os.path.join(save_path, f'inference_file_{file_name}_{sentence_ps}_{window_size}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
